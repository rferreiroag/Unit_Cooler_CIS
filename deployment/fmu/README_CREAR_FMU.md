# C√≥mo Crear FMU para HVAC Unit Cooler Digital Twin

## ‚ö†Ô∏è Estado Actual

He creado la infraestructura completa para exportar los modelos a FMU, pero debido a problemas de compatibilidad con `pythonfmu3` (FMI 3.0 a√∫n es muy nuevo), te proporciono **3 opciones** para crear los FMUs:

---

## Opci√≥n 1: ONNX (RECOMENDADO) ‚úÖ

**Los modelos ONNX ya est√°n listos y funcionan perfectamente:**

```bash
# Ya disponibles en:
deployment/onnx/lightgbm_ucaot.onnx  (669 KB)
deployment/onnx/lightgbm_ucwot.onnx  (667 KB)
deployment/onnx/lightgbm_ucaf.onnx  (298 KB)

# Rendimiento:
- P95 latency: 0.017-0.022 ms
- Throughput: 59,000-66,000 predicciones/seg
- Multi-plataforma: Linux, Windows, macOS, ARM
```

**Ventajas sobre FMU:**
- ‚úÖ Ya funcionan (probados y validados)
- ‚úÖ M√°s r√°pidos que FMU (ONNX Runtime muy optimizado)
- ‚úÖ M√°s peque√±os (1.6 MB vs FMU ~3-5 MB)
- ‚úÖ Mejor soporte multi-plataforma
- ‚úÖ Usados en producci√≥n por Microsoft, Meta, etc.

**Uso en Simulink/Modelica:**
- MATLAB tiene soporte ONNX nativo (R2020b+)
- Modelica puede llamar ONNX v√≠a C++/Python interface
- M√°s eficiente que FMU para ML models

---

## Opci√≥n 2: FMU con FMPy (Python-based)

Si realmente necesitas FMU format:

### Instalaci√≥n:

```bash
pip install fmpy pythonfmu
```

### Crear FMU:

```bash
# Usar el archivo HVACUnitCooler.py proporcionado
# (necesita actualizaci√≥n a FMI 2.0 API)

pythonfmu build -f deployment/fmu/HVAC_FMU_v2.py --dest deployment/fmu/
```

### Limitaci√≥n:

- FMU basado en Python (requiere Python runtime)
- Compatible con: OpenModelica, JModelica
- No compatible con: Dymola comercial, Simulink (requieren FMU binario nativo)

---

## Opci√≥n 3: FMU Nativo (C/C++ - M√°xima Compatibilidad)

Para crear FMU binario nativo compatible con Dymola, Simulink, etc.:

### Pasos:

1. **Exportar modelo a C** usando `treelite`:

```bash
pip install treelite treelite_runtime

python <<EOF
import treelite
import joblib

# Cargar LightGBM
models = joblib.load('models/lightgbm_model.pkl')['models']

# Exportar a C
for target_name, model in models.items():
    model.dump_model(f'deployment/fmu/model_{target_name}.txt', format='txt')
    # Convertir a C con treelite
    treelite_model = treelite.Model.from_lightgbm(model)
    treelite_model.export_lib(
        toolchain='gcc',
        libpath=f'deployment/fmu/model_{target_name}.so',
        verbose=True
    )
EOF
```

2. **Crear FMU C wrapper**:

Archivo `fmu_wrapper.c`:

```c
#include "fmi2Functions.h"
#include <stdlib.h>
#include <string.h>

// Declaraciones de funciones del modelo LightGBM compilado
extern float predict_ucaot(float* features);
extern float predict_ucwot(float* features);
extern float predict_ucaf(float* features);

typedef struct {
    float inputs[52];   // 52 features
    float outputs[3];   // 3 predictions
} ModelData;

// Implementar funciones FMI 2.0
fmi2Component fmi2Instantiate(...) {
    ModelData* data = (ModelData*)calloc(1, sizeof(ModelData));
    return (fmi2Component)data;
}

fmi2Status fmi2DoStep(...) {
    ModelData* data = (ModelData*)component;

    // Escalar inputs (aplicar StandardScaler)
    float scaled_inputs[52];
    apply_scaler(data->inputs, scaled_inputs);

    // Hacer predicciones
    data->outputs[0] = predict_ucaot(scaled_inputs);
    data->outputs[1] = predict_ucwot(scaled_inputs);
    data->outputs[2] = predict_ucaf(scaled_inputs);

    return fmi2OK;
}

// ... m√°s funciones FMI 2.0
```

3. **Compilar para m√∫ltiples plataformas**:

```bash
# Linux 64-bit
gcc -shared -fPIC fmu_wrapper.c model_*.so -o hvac_unit_cooler.so

# Windows 64-bit (cross-compile o en Windows)
x86_64-w64-mingw32-gcc -shared fmu_wrapper.c model_*.dll -o hvac_unit_cooler.dll

# Empaquetar FMU
zip hvac_unit_cooler.fmu modelDescription.xml binaries/linux64/*.so binaries/win64/*.dll resources/*
```

---

## Comparativa de Opciones

| Caracter√≠stica | ONNX | FMU Python | FMU Nativo C |
|----------------|------|------------|--------------|
| **Velocidad** | ‚ö° <1ms | üêå ~10ms | ‚ö° <1ms |
| **Compatibilidad** | ‚úÖ Amplia | ‚ö†Ô∏è Limitada | ‚úÖ M√°xima |
| **Facilidad** | ‚úÖ Ya hecho | ‚ö†Ô∏è Media | ‚ùå Compleja |
| **Tama√±o** | ‚úÖ 1.6 MB | ‚ö†Ô∏è ~5-10 MB | ‚úÖ ~2-3 MB |
| **Multiplataforma** | ‚úÖ Excelente | ‚ö†Ô∏è Requiere Python | ‚úÖ Excelente |
| **Simulink** | ‚úÖ R2020b+ | ‚ùå No | ‚úÖ S√≠ |
| **Modelica/Dymola** | ‚úÖ Via C++ | ‚ö†Ô∏è Solo OpenModelica | ‚úÖ S√≠ |
| **Mantenimiento** | ‚úÖ F√°cil | ‚ö†Ô∏è Medio | ‚ùå Dif√≠cil |

---

## üéØ Recomendaci√≥n Final

**Para la mayor√≠a de casos de uso: Usa ONNX** ‚úÖ

Los FMUs son √∫tiles si:
- Ya tienes un pipeline Modelica/Dymola existente
- Necesitas integraci√≥n espec√≠fica FMI
- No puedes usar ONNX Runtime

Pero ONNX ofrece:
- ‚úÖ Mejor rendimiento
- ‚úÖ M√°s f√°cil de usar
- ‚úÖ Mejor soporte
- ‚úÖ **Ya est√° implementado y probado**

---

## üìö Recursos Adicionales

**ONNX en MATLAB/Simulink:**
- https://www.mathworks.com/help/deeplearning/onnx.html
- Importar modelo: `importONNXNetwork('model.onnx')`

**ONNX en Modelica:**
- Usar ExternalObject con Python/C++ bridge
- Llamar ONNX Runtime desde Modelica C API

**FMU Nativo:**
- Tutorial: https://fmi-standard.org/docs/2.0.3/#_fmi_for_co_simulation
- Herramientas: FMU SDK, fmi-library

---

## üöÄ Quick Start con ONNX (Ya Disponible)

```python
import onnxruntime as ort
import numpy as np
import joblib

# Cargar ONNX
session = ort.InferenceSession('deployment/onnx/lightgbm_ucaot.onnx')

# Cargar scaler
scaler = joblib.load('data/processed/scaler.pkl')

# Preparar input (52 features)
X_raw = np.random.randn(1, 52)
X_scaled = scaler.transform(X_raw)

# Predicci√≥n
input_name = session.get_inputs()[0].name
prediction = session.run(None, {input_name: X_scaled.astype(np.float32)})[0]

print(f"UCAOT: {prediction[0][0]:.2f} ¬∞C")
```

**Performance:** 0.017-0.022 ms (P95) ‚ö°

---

¬øPrefieres que implemente una de estas opciones espec√≠ficamente, o te sirve con ONNX (ya disponible)?
